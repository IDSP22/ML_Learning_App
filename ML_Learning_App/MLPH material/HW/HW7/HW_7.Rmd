---
title: "HW 7"
author: "Dr. Yang Feng"
output:
  html_document:
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warnings = FALSE, fig.align = 'center',  eval = TRUE)
```

You can run the following code to prepare the analysis.
```{r}
library(r02pro)     #INSTALL IF NECESSARY
library(tidyverse)  #INSTALL IF NECESSARY
library(MASS)
my_ahp <- ahp %>% dplyr::select(gar_car, liv_area, lot_area, oa_qual, sale_price) %>%
  na.omit() %>%
  mutate(type = factor(ifelse(sale_price > median(sale_price), "Expensive", "Cheap")))
tr_ind <- 1:(nrow(my_ahp)/20)
my_ahp_train <- my_ahp[tr_ind, ]
my_ahp_test <- my_ahp[-tr_ind, ]
```

Suppose we want to use tree, bagging, random forest, and boosting to predict `sale_price` and `type` using variables `gar_car`, `liv_area`, `lot_area`, and `oa_qual`. Please answer the following questions.


1. Predict `sale_price` (a continuous response) using the training data `my_ahp_train` with tree (with CV pruning), bagging, random forest, and boosting (with CV for selecting the number of trees to be used). For each method, compute the training and test MSE. 
(For boosting, please set `n.trees = 5000, interaction.depth = 1, cv.folds = 5`)

2. Predict `type` (a binary response) using the training data `my_ahp_train` with tree (with CV pruning), bagging, random forest, and boosting (with CV for selecting the number of trees to be used). For each method, compute the training and test classification error. 
(For boosting, please set `n.trees = 5000, interaction.depth = 1, cv.folds = 5`)

3. Question 8.4.2 on Page 362 in ISLRv2. 

4. Question 8.4.5 on Page 362 in ISLRv2. 
