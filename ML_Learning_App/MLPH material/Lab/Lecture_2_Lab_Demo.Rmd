---
title: "Machine Learning in Public Health"
author: "Dr. Yang Feng"
subtitle: 'Lecture 2: Linear Regression (Lab) - Demo'
output:
  html_document:
    pandoc_args: --lua-filter=color-text.lua
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warnings = FALSE, fig.align = 'center',  eval = TRUE)
```

```{r dependencies, eval=TRUE,warning=FALSE}
library(MASS)   
library(tidyverse)   #INSTALL IF NEEDED
library(caret)       #INSTALL IF NEEDED
```

# Data Exploration

Example: Boston housing data

- Output (response, target, dependent) variable is [medv]{color="red"}, the median home price in neighborhoods of Suburbs of Boston
- Input variables include (but not limited to)
	* [crim]{color="blue"}: per capita crime rate by town
	* [rm]{color="blue"}: average number of rooms per dwelling
	* [zn]{color="blue"}: proportion of large lots (zoned for > $25,000$ feet)
	* [chas]{color="blue"}: whether a home is near the Charles river ($x \in \{0, 1\}$)
	* [ptratio]{color="blue"}: pupil-teacher ratio by town


- Let's do a little data exploration

```{r,echo=TRUE,eval=TRUE,warning=FALSE}
data("Boston")
boston <- Boston %>% select(crim, rm, zn, chas, ptratio, medv)
head(boston)
summary(boston)
```

# Simple Linear Regression
## Boston housing: Number of Bedrooms

Idea: let us run a regression on each of these, and see which is best, first, the number of bedrooms `rm`.

```{r,echo=TRUE,eval=TRUE,warning=FALSE}
fit_rm <- lm(medv ~ rm, data = boston)
fit_rm
```

## Measuring the fit

Compute its fitted values $\hat y_i$ and residuals
```{r, echo = TRUE}
df_rm <- data.frame(true_response = boston$medv,
                    fitted_response = fit_rm$fitted.values,
                    residual = fit_rm$residuals)
head(df_rm)
summary(fit_rm)$r.squared
```

## Visualization


```{r,fig.height = 8, fig.width = 14}
# theme function: customize the non-data components of your plots: i.e. titles, labels, fonts, background, gridlines, and legends.
mytheme <- theme(axis.title = element_text(size = 30),
        axis.text = element_text(size = 20))
ggplot(data = boston, mapping = aes(x = rm, y = medv)) + 
  geom_point(size = 2) + 
  geom_smooth(method = "lm", se = FALSE, size = 3) +
  mytheme
ggplot(data = df_rm) +
  geom_point(mapping = aes(x = true_response,
                           y = fitted_response)) +
  mytheme
```

## Boston housing: Charles River


```{r, echo = TRUE, warning=FALSE}
fit_chas <- lm(medv ~ chas, data = boston)
fit_chas
summary(fit_chas)$r.squared
```

```{r}
ggplot(data = boston, mapping = aes(x = chas, y = medv)) + 
  geom_point(size = 2) + 
  geom_smooth(method = "lm", se = FALSE, size = 3) +
  mytheme
```

Its $R^2=$ `r summary(fit_chas)$r.squared`


## Boston housing: Student to teacher ratio


```{r,echo=TRUE,warning=FALSE}
fit_ptratio <- lm(medv ~ ptratio, data = boston)
fit_ptratio
summary(fit_ptratio)$r.squared
```

```{r}
ggplot(data = boston, mapping = aes(x = ptratio, y = medv)) + 
  geom_point(size = 2) + 
  geom_smooth(method = "lm", se = FALSE, size = 3) +
  mytheme
```

# Multiple Linear Regression
## Boston housing: number of bedrooms and student to teacher ratio


```{r,echo=TRUE,warning=FALSE}
fit_rm_ptratio <- lm(medv ~ rm + ptratio, data = boston)
fit_rm_ptratio
summary(fit_rm_ptratio)$r.squared
```

## Housing data: regression on everything
\footnotesize
```{r,echo=TRUE,eval=TRUE,warning=FALSE}
fit_all <- lm(medv ~., data = boston)
summary(fit_all)
```

```{r}
df_all <- data.frame(true_response = boston$medv,
                     fitted_response = fit_all$fitted.values,
                     residual = fit_all$residuals)
df_all <- rbind(cbind(df_rm, type = "rm"),
                cbind(df_all, type = "all"))
```

```{r}
ggplot(df_all) + 
  geom_point(mapping = aes(x = true_response,
                           y = fitted_response,
                           color = type),
             size = 2) +
  geom_abline(slope = 1, intercept = 0, size = 3) + 
  mytheme
```

## Summary of `lm`


```{r,echo=TRUE,eval=TRUE,warning=FALSE}
summary(fit_rm_ptratio)
```

## `ANOVA`

```{r,echo=TRUE,eval=TRUE,warning=FALSE}
anova(fit_rm_ptratio)
```

- The **degrees of freedom** (df) for residuals is $n-p-1$
- the total df = 1 + 1 + 503 = 505
- What's the sample size of this dataset? 

Ans: 506

## ANOVA: reduce model vs. full model

- Reduced model: `medv ~ 1` (no variable)
- Full model: `medv ~ rm + ptratio`
```{r}
fit_null <- lm(medv ~ 1, data = boston)
anova(fit_null, fit_rm_ptratio)
```

## CI and Predicting using the linear regression model  

```{r,echo=TRUE,eval=TRUE,warning=FALSE}
confint(fit_rm_ptratio, level = 0.95) # 95% CI for the coefficients 
```

```{r,echo=TRUE,eval=TRUE,warning=FALSE}
df_test <- data.frame(rm = c(1:5), ptratio = c(2:6))
predict(fit_rm_ptratio, newdata = df_test,
        level = 0.95, interval = "confidence")
```

```{r,echo=TRUE,eval=TRUE,warning=FALSE}
predict(fit_rm_ptratio, newdata = df_test,
        level = 0.95, interval = "prediction")
```

# Qualitative Predictors

- We usually code qualitative variables by **dummy variables** (variables taking values $0$ and $1$). 
```{r}
boston$rm_cate <- cut_number(boston$rm, 3)
levels(boston$rm_cate) <- c("Small", "Medium", "Big")
head(boston %>% select(medv, rm, rm_cate))
```

- R will automatically do the conversion for us in `lm()`
```{r}
fit_rm_cate <- lm(medv ~ rm_cate, data = boston)
summary(fit_rm_cate)
```

# $K$ Nearest Neighbors

## KNN: Generate Data
- Recall we assume $Y = f(X) + \epsilon$. 
```{r}
x <- seq(from = -6, to = 6, by = 0.1)
n <- length(x)
y_true <- sin(x)    #true function
y <- y_true + rnorm(length(x))
train_dat <- data.frame(x = x, y = y, y_true = y_true)
ggplot(train_dat, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red", se = FALSE,
              size = 2) +
  geom_line(mapping = aes(x = x, y = y_true), 
            color = "purple",
            size = 2) + mytheme
head(train_dat)
```

## KNN: Different choice of $K$


```{r}
x_test <- seq(from = -6, to = 6, by = 0.01)  #test data
y_true_test <- sin(x_test)
y_test <- y_true_test + rnorm(length(x_test))
k_seq <- c(1, 10, n)
pred_dat <- NULL
test_error_seq <- train_error_seq <- rep(0, 3)
for(k in seq_along(k_seq)){
  # knnreg: $k$-nearest neighbour regression that can return the average value for the neighbours.
  fit <- knnreg(data.frame(x), y, k = k_seq[k])
  y_test_hat <- predict(fit, data.frame(x_test))
  test_error_seq[k] <- sum((y_test - y_test_hat)^2)
  y_train_hat <- predict(fit, data.frame(x))
  train_error_seq[k] <- sum((y - y_train_hat)^2)
  pred_dat <- rbind(pred_dat,
                    data.frame(x = x_test,
                               y = y_test_hat,
                               y_true = y_test,
                               k = k_seq[k]))
}
data.frame(k = k_seq, train_error = train_error_seq, test_error = test_error_seq)
# note: multiple datasets in one plot
ggplot(pred_dat) + 
  geom_point(mapping = aes(x = x,
                           y = y),
             data = train_dat,
             alpha = 0.5,
             color = "black") + 
  geom_point(mapping = aes(x = x, 
                           y = y,
                           color = factor(k)),
             alpha = 0.5) +
  geom_line(mapping = aes(x = x,
                          y = y_true),
            data = train_dat,
            size = 2) 
```

## KNN Bonus Exercise

 - Simulate the data using LDA
 
```{r}
n <- 300
set.seed(0)
x1 <- rnorm(n/2, 0)
x2 <- rnorm(n/2, 1)
y1 <- rep(0,n/2)
y2 <- rep(1,n/2)
dat <- data.frame(x = c(x1, x2), y = c(y1, y2))
 
```

```{r}
##Splitting the dataset into training and testing 
rand_ind = sample(1:n)
tr_ind = rand_ind[1:(n/2)]
te_ind = rand_ind[(n/2+1):n]
```

```{r}
library(class)
##Fitting KNN with K=3
knn.train.pred = knn(dat[tr_ind, 1, drop = FALSE],
                      dat[tr_ind, 1, drop = FALSE], 
                      dat$y[tr_ind], k=3)
 
knn.train.pred = knn(data.frame(dat[tr_ind,]$x),
                      data.frame(dat[tr_ind,]$x), 
                      dat$y[tr_ind], k=3)
mean(knn.train.pred != dat$y[tr_ind]) ##Training Error
knn.test.pred = knn(data.frame(dat[tr_ind,]$x),
                      data.frame(dat[te_ind,]$x), 
                      dat$y[tr_ind], k=3)
mean(knn.test.pred != dat$y[te_ind]) ##Test Error
``` 

```{r}
##Fitting KNN with K=10
knn.train.pred = knn(data.frame(dat[tr_ind,]$x),
                      data.frame(dat[tr_ind,]$x), 
                      dat$y[tr_ind], k=10)
mean(knn.train.pred != dat$y[tr_ind]) ##Training Error
knn.test.pred = knn(data.frame(dat[tr_ind,]$x),
                      data.frame(dat[te_ind,]$x), 
                      dat$y[tr_ind], k=10, prob = TRUE)
mean(knn.test.pred != dat$y[te_ind]) ##Test Error

```

```{r}
###Find which K is the best for KNN
#Step 1: split tr_ind into training and validation
train_ind <- tr_ind[1:100]
val_ind <- tr_ind[101:150]
 
#Step 2: Go over each choice of K and find the optimal one
K_seq <- seq(from = 1, to = 99, by = 2)
len <- length(K_seq)
val_err_seq <- rep(0,len)
for(j in 1:len){
  K <- K_seq[j]
  knn.test.pred = knn(data.frame(dat[train_ind,]$x),
                      data.frame(dat[val_ind,]$x), 
                      dat$y[train_ind], k = K)
  val_err_seq[j] <- mean(knn.test.pred != dat$y[val_ind]) 
  ##Validation Error
}
par(mfrow=c(1,1))
plot(K_seq, val_err_seq)
opt_ind <- max(which(val_err_seq == min(val_err_seq)))
opt_K <- K_seq[opt_ind]
 
 
#Step 3: Use the optimal K to redo the prediction 
knn.test.pred = knn(data.frame(dat[tr_ind,]$x),
                    data.frame(dat[te_ind,]$x), 
                    dat$y[tr_ind], k = opt_K)
mean(knn.test.pred != dat$y[te_ind]) ##Test Error
```




