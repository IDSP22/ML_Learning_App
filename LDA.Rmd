---
title: "LDA"
output: html_document
---

### Two Different Modeling Approaches 
* Logistic regression models $P(Y = 1|X)$ by the logistic function
* Linear discriminant analysis (LDA) models the conditional distribution of X given Y by Gaussian (a.k.a. normal) distributions
* Bayes' Theorem
  + Suppose $π_k$ denotes prior probability $P(Y = k)$ of the kth class
  + Suppose $f_k(X)$ denotes the density function of X for an observation that comes from the kth class (i.e. $f_k$ is the p.d.f. of $(X|Y = k)$)
  + Then we have $$P(Y = k|X = x) = \frac{π_kf_k(X)}{\sum {l = 1}^{K}π_1f_1(x)}$$
  where K is the total number of classes
  
### Bayes Classifier
* If a classiﬁer assigns an observation x to class k that has the largest $$p_k(x) = P(Y = k|X = x)$$ , this classiﬁer has the minimum classiﬁcation error (Bayes classiﬁer, a.k.a. oracle classiﬁer)
* In other words, Bayes classiﬁer assign x to arg max $π_kf_k(x)$
* Often, estimating $π_k$ ’s is straightforward; but estimating $f_k$ involves some technicality

### Linear Discriminant Analysis (LDA)(for $p=1$)
* In LDA, we assume that $f_k(x)$ ’s are normally distributed, and they have the same standard deviation coefﬁcient:
$$$ = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2$$
* Under this assumption, it can be shown that assigning an observation to a class according to the largest $p_k (x)$ is the same as assigning to the observation according to the largest
$$\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu^2_{k}{2\sigma^2}} + log(\pi_k)$$
* Note that the $\delta_k$ function is linear in x. The LDA approximates $\delta_k(x)$ by pluggin estimates for $\pi_k$, $\mu_k$, and $\sigma_k$:
$$\hat\pi_k = n_k/n, \hat\mu_k = \frac{1}{n_k}\sum{i:yi=k}X_i$$
  where $n_k$ is the number of training observations in the kth class
$$\hat\sigma^2 = \frac{1}{n-K}\sum{k=1}^K\sum{i:yi=k}(x_i-\hat\mu_k)^2$$

### Linear Discriminant Analysis (LDA)
* So for $p=1$ the **discriminant function** is
$$\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu^2_{k}{2\sigma^2}} + log(\pi_k)$$
  $\delta_k(x)$ is linear in x
* LDA classiﬁer: assign x to class k for the largest $\delta_k(x)$
* Question: for K = 2 , show LDA has a linear decision boundary

### Linear Discriminant Analysis (LDA, p >1)
* When $p > 1$, assume $f_k(x)\sim N(\mu_k, \sum)$, a multivariate normal distribution with mean $\mu_k$ and covariance matrix $\sum$ in ISLR is the probability density function of a multivariate normal variable)
* Bayes classiﬁer assigns an observation to class k for the largest
$$\delta_k(x) = x^T\sum^{-1}\mu_k-\frac{1}{2}\mu_{k}^{T}\sum^{-1}\mu_k+log\pi_k$$
  Again, need to plug-in estimates of $\mu_k$, $\sum$, and $\pi_k$